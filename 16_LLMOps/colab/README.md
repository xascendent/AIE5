<p align = "center" draggable=â€falseâ€ ><img src="https://github.com/AI-Maker-Space/LLM-Dev-101/assets/37101144/d1343317-fa2f-41e1-8af1-1dbb18399719" 
     width="200px"
     height="auto"/>
</p>

## <h1 align="center" id="heading">LangSmith & LangGraph Studio</h1>

### [Quicklinks](https://github.com/AI-Maker-Space/AIE5/00_AIM_Quicklinks)

| ðŸ¤“ Pre-work | ðŸ“° Session Sheet | âºï¸ Recording     | ðŸ–¼ï¸ Slides        | ðŸ‘¨â€ðŸ’» Repo         | ðŸ“ Homework      | ðŸ“ Feedback       |
|:-----------------|:-----------------|:-----------------|:-----------------|:-----------------|:-----------------|:-----------------|
| [Session 16: Pre-Work](https://www.notion.so/Session-16-Deploying-and-Operating-RAG-in-Production-189cd547af3d80bfb5d4fbd5dcf74699?pvs=4#1aecd547af3d80208a18e0bcb3a95c50)| [Session 16: Deploying and Operating RAG in Production](https://www.notion.so/Session-16-Deploying-and-Operating-RAG-in-Production-189cd547af3d80bfb5d4fbd5dcf74699) | [Recording](https://us02web.zoom.us/rec/share/-hIkx9NLIftJq6ms1wSU_dQb9OrEqrAyFkiv3IAQ5wRXjhKL55dSJGGW0ZQAwmck.HsK8jCAhrJ2WnbwW)  (?3pQAA1g)| [Session 16: LLM Ops: Deploying to & Operating in Production](https://www.canva.com/design/DAGg-IJ-Ezw/d2Bi3DBrOUr74odnIEtRqQ/edit?utm_content=DAGg-IJ-Ezw&utm_campaign=designshare&utm_medium=link2&utm_source=sharebutton) | You Are Here!| [Session 16 Assignment: Deploying and Operating RAG in Production](https://forms.gle/wWjwSRjbcNRiR8cWA)| [AIE5 Feedback 3/6](https://forms.gle/CVwrtQLEfMNa4wAfA) |


# Build ðŸ—ï¸

Run the notebook ([Colab link](https://colab.research.google.com/drive/1YsYbNpH7VrqAGdv8aBWGuRDu8biknL_w?usp=sharing)) and complete the following:

1. ðŸ¤ BREAKOUT ROOM #1:
  - Task 1: Set Up HF Endpoints As in Session 15
  - Task 2: Depends and Set-Up
  - Task 3: Setting up RAG With Production in Mind
  - Task 4: RAG LCEL Chain
2. ðŸ¤ BREAKOUT ROOM #2:
  - Deploy LangGraph Studio Application Using Resources in `open_deep_research` folder. 

## ADVANCED BUILD:

The caching we're using is both: 

1. Ineffecient
2. Exact Match

Please produce a locally running application (through Docker) that integrates a more intelligent caching process.

In simpler terms: 

- Use a database approach (Redis, Vectordatase, SQLite, etc.) instead of plain-memory for caching
- Implement Semantic LLM Caching OR Implement E2E Caching

> NOTE: Doing the advanced build will count as your assignment for the week. If you do the advanced build, you are not required to do the notebook.

# Ship ðŸš¢

- HF App.
- 5min. Loom Video

# Share ðŸš€
- Walk through your notebook and explain what you've completed in the Loom video
- Make a social media post about your final application and tag @AIMakerspace
- Share 3 lessons learned
- Share 3 lessons not learned
