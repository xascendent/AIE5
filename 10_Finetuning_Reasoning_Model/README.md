<p align = "center" draggable=”false” ><img src="https://github.com/AI-Maker-Space/LLM-Dev-101/assets/37101144/d1343317-fa2f-41e1-8af1-1dbb18399719" 
     width="200px"
     height="auto"/>
</p>

## <h1 align="center" id="heading">Session 10: Fine-tuning a Reasoning Model</h1>

| 🤓 Pre-work | 📰 Session Sheet | ⏺️ Recording     | 🖼️ Slides        | 👨‍💻 Repo         | 📝 Homework      | 📁 Feedback       |
|:-----------------|:-----------------|:-----------------|:-----------------|:-----------------|:-----------------|:-----------------|
| [Session 10: Pre-Work](https://www.notion.so/Session-10-Fine-Tuning-LLM-Chat-Models-189cd547af3d80b3b460cdf16ceb212d?pvs=4#189cd547af3d81679890c02cf1812071)| [Session 10: Fine-Tuning LLMs & Reasoning Models ](https://www.notion.so/Session-10-Fine-Tuning-LLM-Chat-Models-189cd547af3d80b3b460cdf16ceb212d) | [Recording](https://us02web.zoom.us/rec/share/WM85q-HkiEMDfHP4j4vXB5dB-f0W9Q39fVWvKNZzqED9HjK89jJMcMNaOUafDDE4.pkaFKBwyFr-ZjzDj) (!hU0a^dx)| [Session 10: Fine-Tuning LLMs & Reasoning Models](https://www.canva.com/design/DAGcIQDgXnI/32F5QSz2_S_8b2ATHjZrBA/edit?utm_content=DAGcIQDgXnI&utm_campaign=designshare&utm_medium=link2&utm_source=sharebutton)| [10_Finetuning_Reasoning_Model](https://github.com/AI-Maker-Space/AIE5/tree/main/10_Finetuning_Reasoning_Model) | [Session 10 Assignment: Fine-Tuning LLMs & Reasoning Models](https://forms.gle/aMDtvaYdbjmtvBuN7)  | [AIE5 Feedback 2/13](https://forms.gle/zQGUnYk5AYfp2v9r9) |

In today's assignment, we'll be fine-tuning Llama 3.1 8B to create a reasoning model!

- 🤝 Breakout Room #1
  - Overview of PEFT and LoRA
  - Installation
  - Unsloth Setup
  - Task 1: Loading the Model
    - Overview of Quantization
    - Block-wise k-bit Quantization
    - Initializing LoRA Config

- 🤝 Breakout Room #2
  - Task 1: Loading Data
  - Task 2: Training Setup
  - Task 3: GRPO Training Loop
  - Task 4: Model Evaluation
    
The notebook Colab link is located [here](https://colab.research.google.com/drive/18jF-pOlz-cFt0SkHVQ_9PBuGAH0VumrG?usp=sharing)

## Ship 🚢

The completed notebook!

#### 🏗️ BONUS ACTIVITY (FULL MARKS IF COMPLETED IN LIEU OF ABOVE NOTEBOOK):

Using the [Open R1 Math Raw](https://huggingface.co/datasets/open-r1/OpenR1-Math-Raw) dataset, fine-tune Llama 3.1 8B with Unsloth to produce a reasoning model using GRPO.

> NOTE: You will need to come up with your own reward functions that correctly reward the model as it learns to reason.

### Deliverables

- A short Loom of the notebook, and a 1min. walkthrough of the application in full

## Share 🚀

Make a social media post about your final application!

### Deliverables

- Make a post on any social media platform about what you built!

Here's a template to get you started:

```
🚀 Exciting News! 🚀

I am thrilled to announce that I have just built and shipped fine-tuning a reasoning model with GRPO! 🎉🤖

🔍 Three Key Takeaways:
1️⃣ 
2️⃣ 
3️⃣ 

Let's continue pushing the boundaries of what's possible in the world of AI and question-answering. Here's to many more innovations! 🚀
Shout out to @AIMakerspace !

#LangChain #QuestionAnswering #RetrievalAugmented #Innovation #AI #TechMilestone

Feel free to reach out if you're curious or would like to collaborate on similar projects! 🤝🔥
```
